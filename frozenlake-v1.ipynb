{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "from lib.params import Params\n",
    "from lib.agent import QLearningAgent\n",
    "from lib.post_process import post_process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10_000\n",
    "initial_epsilon = 1\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=total_episodes,\n",
    "    learning_rate=0.8,\n",
    "    discount_factor=0.95,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    epsilon_decay=initial_epsilon / (total_episodes / 2), # reduce the exploration over time\n",
    "    final_epsilon=0.1,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"fig/frozenlake-v1/\"),\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# create the figure folder if it doesn't exists\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, agent: QLearningAgent):\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # run several times to account for stochasticity\n",
    "        agent.reset_qtable()  # reset the Q-table between runs\n",
    "\n",
    "        for episode in tqdm(\n",
    "            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "        ):\n",
    "            state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            # play episode\n",
    "            while not done:\n",
    "                action = agent.get_action(\n",
    "                     state=state, \n",
    "                     action_space=env.action_space\n",
    "                )\n",
    "\n",
    "                # log all states and actions\n",
    "                all_states.append(state)\n",
    "                all_actions.append(action)\n",
    "\n",
    "                # take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # update the agent\n",
    "                agent.update(\n",
    "                    state=state, \n",
    "                    action=action, \n",
    "                    reward=reward,\n",
    "                    terminated=terminated, \n",
    "                    new_state=new_state\n",
    "                )\n",
    "\n",
    "                total_rewards += reward\n",
    "                step += 1\n",
    "\n",
    "                # new state is now state\n",
    "                state = new_state\n",
    "\n",
    "            # log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "        qtables[run, :, :] = agent.qtable\n",
    "\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "\n",
    "Running the agent on a few increasing maps sizes: 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map size: 4x4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "map_sizes = [4]\n",
    "res_all = pd.DataFrame()\n",
    "st_all = pd.DataFrame()\n",
    "\n",
    "for map_size in map_sizes:\n",
    "    env = gym.make(\n",
    "        \"FrozenLake-v1\",\n",
    "        is_slippery=params.is_slippery,\n",
    "        render_mode=\"rgb_array\",\n",
    "        desc=generate_random_map(\n",
    "            size=map_size, p=params.proba_frozen, seed=params.seed\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    params = params._replace(action_size=env.action_space.n)\n",
    "    params = params._replace(state_size=env.observation_space.n)\n",
    "    env.action_space.seed(\n",
    "        params.seed\n",
    "    )  # set the seed to get reproducible results when sampling the action space\n",
    "    \n",
    "    agent = QLearningAgent(\n",
    "        learning_rate=params.learning_rate,\n",
    "        discount_factor=params.discount_factor,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "        initial_epsilon=params.initial_epsilon, \n",
    "        epsilon_decay=params.epsilon_decay, \n",
    "        final_epsilon=params.final_epsilon, \n",
    "        rng_seed=params.seed\n",
    "    )\n",
    "\n",
    "    print(f\"Map size: {map_size}x{map_size}\")\n",
    "    rewards, steps, episodes, qtables, all_states, all_actions = train(env, agent)\n",
    "\n",
    "    # process results and add to dataframe\n",
    "    res, st = post_process(episodes, params, rewards, steps, map_size)\n",
    "    res_all = pd.concat([res_all, res])\n",
    "    st_all = pd.concat([st_all, st])\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
